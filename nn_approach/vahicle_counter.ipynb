{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed setting\n",
    "np.random.seed(42)"
   ]
  },
  {
   "source": [
    "# Neural Network Approach"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load YOLO object detector trained on COCO dataset (80 classes)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"yolo_config/yolov3.weights\"\n",
    "config_path = \"yolo_config/yolov3.cfg\"\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(\n",
    "    config_path, \n",
    "    weights_path\n",
    ")\n",
    "\n",
    "INPUT_WIDTH, INPUT_HEIGHT = 416, 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of all layers of the network\n",
    "ln = net.getLayerNames()\n",
    "# Get the index of the output layers\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "source": [
    "## Determine only the *output* layer names that we need from YOLO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_LABELS = open(\"yolo_config/coco.names\").read().strip().split(\"\\n\")\n",
    "\n",
    "CHOSEN_LABELS = [\"bicycle\", \"car\", \"motorbike\", \"bus\", \"truck\", \"train\"]\n",
    "\n",
    "# Initialize a list of colors to represent each possible class label\n",
    "LABEL_COLORS = np.random.randint(\n",
    "    0, 255, \n",
    "    size=(len(COCO_LABELS), 3), \n",
    "    dtype=\"uint8\"\n",
    ")"
   ]
  },
  {
   "source": [
    "## Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Display the vehicle count on the top-left corner of the frame\n",
    "# PARAMETERS: \n",
    "#   - frame - frame on which the count is displayed\n",
    "#   - vehicle_count - number of vehicles\n",
    "def display_vehicle_count(vehicle_count, frame):\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        'Detected Vehicles: ' + str(vehicle_count), \n",
    "        (20, 20), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        0.8, \n",
    "        (0, 0xFF, 0), \n",
    "        2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Display the FPS of the detected video\n",
    "# PARAMETERS: \n",
    "#   - start_time - start time of the frame\n",
    "#   - num_frames - number of frames within the same second\n",
    "# RETURN: \n",
    "#   - start_time - new start time\n",
    "#   - num_frames - new number of frames\n",
    "def display_FPS(start_time, num_frames):\n",
    "    current_time = int(time.time())\n",
    "\n",
    "    if current_time > start_time:\n",
    "        os.system('clear')  # Equivalent of CTRL+L on the terminal\n",
    "        print(\"FPS:\", num_frames)\n",
    "        num_frames = 0\n",
    "        start_time = current_time\n",
    "\n",
    "    return start_time, num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Draw all the detection boxes with a green dot at the center\n",
    "# PARAMETERS: \n",
    "#   - box_ids - detection box ids\n",
    "#   - boxes - all bounding boxes\n",
    "#   - class_ids - detection class ids\n",
    "#   - confidences - detection confidences\n",
    "#   - frame - frame on which the detection boxes are displayed\n",
    "def draw_detected_boxes(box_ids, boxes, class_ids, confidences, frame):\n",
    "    if len(box_ids) > 0:\n",
    "        for i in box_ids.flatten():\n",
    "            # Extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            # Draw a bounding box rectangle and label on the frame\n",
    "            color = [int(c) for c in LABEL_COLORS[class_ids[i]]]\n",
    "\n",
    "            cv2.rectangle(\n",
    "                frame, \n",
    "                (x, y), \n",
    "                (x + w, y + h), \n",
    "                color, \n",
    "                2\n",
    "            )\n",
    "\n",
    "            text = \"{}: {:.4f}\".format(\n",
    "                COCO_LABELS[class_ids[i]],\n",
    "                confidences[i]\n",
    "            )\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                text, \n",
    "                (x, y - 5), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                color, \n",
    "                2\n",
    "            )\n",
    "\n",
    "            # Draw a green dot in the middle of the box\n",
    "            cv2.circle(\n",
    "                frame, \n",
    "                (x + (w // 2), y + (h // 2)), \n",
    "                2, \n",
    "                (0, 0xFF, 0), \n",
    "                2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Draw detection box id on its center\n",
    "# PARAMETERS: \n",
    "#   - current_box_id - current box id\n",
    "#   - current box center - current box center\n",
    "#   - frame - frame on which the detection boxes are displayed\n",
    "def draw_detection_box_id(current_box_id, current_box_center, frame):\n",
    "    cv2.putText(\n",
    "        frame, \n",
    "        current_box_id, \n",
    "        current_box_center, \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        0.5, \n",
    "        [0, 0, 255], \n",
    "        2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Initializing the video writer with the output video path and the same number\n",
    "#   of fps, width and height as the source video\n",
    "# PARAMETERS: \n",
    "#   - video_width - video width\n",
    "#   - video_height - video height\n",
    "#   - video_cap - video stream\n",
    "# RETURN: The initialized video writer\n",
    "def initialize_video_writer(video_width, video_height, video_cap):\n",
    "    # Getting the fps of the source video\n",
    "    fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "    size = (video_width, video_height)\n",
    "\n",
    "    video_writer = cv2.VideoWriter(\n",
    "        video_mp4_out_path, \n",
    "        cv2.VideoWriter_fourcc(*'XVID'), \n",
    "        fps, \n",
    "        size)\n",
    "\n",
    "    return video_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Identifying if the current box was present in the previous frames\n",
    "# PARAMETERS: \n",
    "#   - prev_frames_detections - all the vehicular detections of N previous frames (N = FRAMES_BEFORE_CURRENT)\n",
    "#   - current_box - current box\n",
    "#   - current_detections - the coordinates of the box of previous detections\n",
    "# RETURN: \n",
    "#   - False - if the box was present in previous frames\n",
    "#\t- True - if the box wasn't present in previous frames (is new)\n",
    "def is_new_object(prev_frames_detections, current_box, current_detections):\n",
    "    center_x, center_y, width, height = current_box\n",
    "\n",
    "    dist = np.inf\n",
    "    # Iterating through all the k-dimensional trees\n",
    "    for i in range(FRAMES_BEFORE_CURRENT):\n",
    "        prev_frame_detections = list(prev_frames_detections[i].keys())\n",
    "        if len(prev_frame_detections) == 0:\n",
    "            continue\n",
    "        # Finding the distance to the closest point and the index\n",
    "        temp_dist, index = spatial.KDTree(prev_frame_detections).query([(center_x, center_y)])\n",
    "        if temp_dist < dist:\n",
    "            dist = temp_dist\n",
    "            frame_num = i\n",
    "            coords = prev_frame_detections[index[0]]\n",
    "\n",
    "    if dist > (max(width, height) / 2):\n",
    "        return True\n",
    "\n",
    "    # Keeping the vehicle ID constant\n",
    "    current_detections[(center_x, center_y)] = prev_frames_detections[frame_num][coords]\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE:              # Identifying if the current box was present in the previous frames\n",
    "# PARAMETERS: \n",
    "#   - box_ids - detection box ids\n",
    "#   - boxes - all bounding boxes\n",
    "#   - class_ids - detection class ids\n",
    "#   - vehicle_count - current vehicle count\n",
    "#   - prev_frames_detections - all the vehicular detections of N previous frames (N = FRAMES_BEFORE_CURRENT)\n",
    "#   - frame - frame on which the detection boxes are displayed\n",
    "# RETURN: \n",
    "#   - vehicle_count - new vehicle count\n",
    "#\t- current_detections - current detections\n",
    "def count_vehicles(box_ids, boxes, class_ids, vehicle_count, prev_frames_detections, detection_zone, frame):\n",
    "    current_detections = {}\n",
    "\n",
    "    if len(box_ids) > 0:\n",
    "        for i in box_ids.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            center_x = x + (w // 2)\n",
    "            center_y = y + (h // 2)\n",
    "\n",
    "            # When the detection is in CHOSEN_LABELS\n",
    "            if COCO_LABELS[class_ids[i]] in CHOSEN_LABELS and \\\n",
    "               center_x < detection_zone[0] and \\\n",
    "               center_y > detection_zone[1]:\n",
    "                current_detections[(center_x, center_y)] = vehicle_count\n",
    "                current_box = (center_x, center_y, w, h)\n",
    "\n",
    "                # When the ID of the detection is not present in previous frames detections\n",
    "                if is_new_object(prev_frames_detections, current_box, current_detections):\n",
    "                    vehicle_count += 1\n",
    "\n",
    "                # Get the id corresponding to the current detection\n",
    "                current_box_id = current_detections[(center_x, center_y)]\n",
    "\n",
    "                # When two detections have the same id (due to being too close)\n",
    "                if list(current_detections.values()).count(current_box_id) > 1:\n",
    "                    current_detections[(center_x, center_y)] = vehicle_count\n",
    "                    vehicle_count += 1\n",
    "                \n",
    "                # Get the id corresponding to the current detection\n",
    "                current_box_id = current_detections[(center_x, center_y)]\n",
    "\n",
    "                # Display the id at the center of the box\n",
    "                current_box_center = (center_x, center_y)\n",
    "                draw_detection_box_id(str(current_box_id), current_box_center, frame)\n",
    "\n",
    "    return vehicle_count, current_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: Draw detection zone \n",
    "# PARAMETERS: \n",
    "#   - detection_zone - detection zone placement\n",
    "#   - frame - frame on which the detection zone is displayed\n",
    "def draw_detection_zone(detection_zone, frame):\n",
    "    cv2.line(\n",
    "        frame, \n",
    "        detection_zone[0],\n",
    "        detection_zone[1], \n",
    "        (255,1,1), \n",
    "        5\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Path handling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'back_view_640'  # bridge, highway, night, highway_day, highway_night, cars_640, back_view_640, night_speed_640\n",
    "video_in_mp4 = video + '.mp4'\n",
    "video_out_avi = video + '.avi'\n",
    "\n",
    "in_dir = '../videos/'\n",
    "out_dir = '../out_videos/yolo/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "video_mp4_in_path = os.path.join(in_dir, video_in_mp4)\n",
    "video_mp4_out_path = os.path.join(out_dir, video_out_avi)"
   ]
  },
  {
   "source": [
    "## Data preperation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def video_cap_describe(video_cap):\n",
    "    print(f'Width: \\t\\t{round(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))}')\n",
    "    print(f'Height: \\t{round(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}')\n",
    "    print(f'FPS: \\t\\t{round(video_cap.get(cv2.CAP_PROP_FPS))}')\n",
    "    print(f'Frames count:\\t{round(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))}')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Width: \t\t640\nHeight: \t360\nFPS: \t\t25\nFrames count:\t1173\n"
     ]
    }
   ],
   "source": [
    "# Initialize the video stream, pointer to output video file\n",
    "video_cap = cv2.VideoCapture(video_mp4_in_path)\n",
    "video_cap_describe(video_cap)\n",
    "\n",
    "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "video_frames_count = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "writer = initialize_video_writer(video_width, video_height, video_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_CONFIDENCE = 0.5\n",
    "DETECTION_THRESHOLD = 0.3\n",
    "FRAMES_BEFORE_CURRENT = round(video_fps / 4)    # (e.g. 60/4 = 15)\n",
    "DISPLAY_WHILE_PROCESSING = True\n",
    "DETECTION_ZONE = [(0,240), (500,240)]\n",
    "DETECTION_ZONE_MAX_X = max(DETECTION_ZONE[0][0], DETECTION_ZONE[1][0])\n",
    "DETECTION_ZONE_MAX_Y = max(DETECTION_ZONE[0][1], DETECTION_ZONE[1][1])"
   ]
  },
  {
   "source": [
    "## Main loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 16%|█▌        | 185/1173 [01:10<06:16,  2.63it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7bb8f9b433d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Loop over each layer outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "prev_frames_detections = [{(0, 0): 0} for i in range(FRAMES_BEFORE_CURRENT)]\n",
    "vehicle_count = 0\n",
    "\n",
    "# Main loop over video frames\n",
    "# while True:\n",
    "for _ in tqdm(range(video_frames_count)):\n",
    "    # Initialization for each iteration\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "    # TODO: detection zone\n",
    "    # vehicle_crossed_line_flag = False\n",
    "\n",
    "    # Read the next video frame\n",
    "    (grabbed, frame) = video_cap.read()\n",
    "\n",
    "    # If the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # Construct a blob from the input frame and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes\n",
    "    # and associated confidences\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        frame, \n",
    "        1 / 255.0, \n",
    "        (INPUT_WIDTH, INPUT_HEIGHT), \n",
    "        swapRB=True, \n",
    "        crop=False\n",
    "    )\n",
    "    net.setInput(blob)\n",
    "    layer_outputs = net.forward(ln)\n",
    "\n",
    "    # Loop over each layer outputs\n",
    "    for output_detections in layer_outputs:\n",
    "        # Loop over each of detection\n",
    "        for i, detection in enumerate(output_detections):\n",
    "            # detection[0:4] return center_x, center_y, width, height\n",
    "            #   detection[5:] returns the confidences score for each class\n",
    "            detection_confidences = detection[5:]\n",
    "            class_id = np.argmax(detection_confidences)\n",
    "            confidence = detection_confidences[class_id]\n",
    "\n",
    "            # Filter out weak predictions\n",
    "            if confidence > DETECTION_CONFIDENCE:\n",
    "                # Scale the bounding box coordinates back relative to\n",
    "                # the size of the image, keeping in mind that YOLO actually \n",
    "                # returns the center (x, y)-coordinates of the bounding box \n",
    "                # followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([video_width, video_height, video_width, video_height])\n",
    "                (center_x, center_y, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # Get the top and left corner of the bounding box\n",
    "                x = int(center_x - (width / 2))\n",
    "                y = int(center_y - (height / 2))\n",
    "\n",
    "                # Update: bounding box coordinates, confidences, and class ids lists\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "                \n",
    "    draw_detection_zone(DETECTION_ZONE, frame)\n",
    "    # TODO: detection zone\n",
    "    # # Changing line color to green if a vehicle in the frame has crossed the line\n",
    "    # if vehicle_crossed_line_flag:\n",
    "    # \tcv2.line(frame, (x1_line, y1_line), (x2_line, y2_line), (0, 0xFF, 0), 2)\n",
    "    # # Changing line color to red if a vehicle in the frame has not crossed the line\n",
    "    # else:\n",
    "    # \tcv2.line(frame, (x1_line, y1_line), (x2_line, y2_line), (0, 0, 0xFF), 2)\n",
    "\n",
    "    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    box_ids = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes, \n",
    "        scores=confidences, \n",
    "        score_threshold=DETECTION_CONFIDENCE, \n",
    "        nms_threshold=DETECTION_THRESHOLD\n",
    "    )\n",
    "\n",
    "    # Draw detection box\n",
    "    draw_detected_boxes(\n",
    "        box_ids=box_ids, \n",
    "        boxes=boxes, \n",
    "        class_ids=class_ids, \n",
    "        confidences=confidences, \n",
    "        frame=frame\n",
    "    )\n",
    "\n",
    "    vehicle_count, current_detections = count_vehicles(\n",
    "        box_ids=box_ids, \n",
    "        boxes=boxes, \n",
    "        class_ids=class_ids, \n",
    "        vehicle_count=vehicle_count, \n",
    "        prev_frames_detections=prev_frames_detections,\n",
    "        detection_zone=(DETECTION_ZONE_MAX_X, DETECTION_ZONE_MAX_Y),\n",
    "        frame=frame\n",
    "    )\n",
    "\n",
    "    # Display Vehicle Count TODO: \"if a vehicle has passed the line\"\n",
    "    display_vehicle_count(vehicle_count, frame)\n",
    "\n",
    "    # Write the output frame to disk\n",
    "    writer.write(frame)\n",
    "\n",
    "    if DISPLAY_WHILE_PROCESSING:\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "    # https://stackoverflow.com/questions/35372700/whats-0xff-for-in-cv2-waitkey1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Updating prev_frames_detections\n",
    "    # Removing the first frame from the list\n",
    "    prev_frames_detections.pop(0)\n",
    "    prev_frames_detections.append(current_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Claenup and save"
   ]
  },
  {
   "source": [
    "writer.release()\n",
    "video_cap.release()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}